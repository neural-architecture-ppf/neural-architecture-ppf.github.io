<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
    <title>ICCV 2021 Workshop on Neural Architectures: Past, Present and Future</title>

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
    <link href='https://fonts.googleapis.com/css?family=Lato:400,700' rel='stylesheet' type='text/css'>
    <link href="css/style.css" rel="stylesheet" type="text/css"/>
</head>

<body>

<<!-- div class="section" align="center">
<h2><a href="index.html">Home</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <a href="sub-sites/call-for-paper.html">Call for papers</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <a href="sub-sites/submission.html">Submission</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <a href="sub-sites/schedule.html">Schedule</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <a href="sub-sites/accepted-paper.html">Accepted Papers</a>
</h2>
</div> -->

<div class="container">
    <table border="0" align="center">
        <tr>
            <td width="700" align="center" valign="middle"><h3>ICCV 2021 Workshop on</h3>
                <span class="title"><strong>Neural Architectures: Past, Present and Future</strong></span></td>
        </tr>
        <tr>
        <td colspan="3" align="center"><h3>Montreal, Canada<br>Full day, October 11th, 2021<br><br>

        Gatherly: <a href="https://workshopsdayone.event.gatherly.io">https://workshopsdayone.event.gatherly.io</a><br>
        YouTube: <a href="https://www.youtube.com/watch?app=desktop&v=EdJsrxJaobU">https://www.youtube.com/watch?app=desktop&v=EdJsrxJaobU</a>
        <!-- https://us06web.zoom.us/j/85399097261?pwd=SCtDUnFLREZkY2craDdqL1B6RlBSdz09 -->
    </table>
</div>

</br>

<div class="container">
    <h2>Overview</h2>
    <div class="overview">
        <p>The surge of deep learning has largely benefited from the success of neural architecture design. By evolving from LeNet to AlexNet to VGG and to ResNet, neural architecture keeps incorporating novel designs of architectural elements and network topologies, leading to significant improvements in representation learning. Recently, the emergence of neural architecture search (NAS) further advances the representational capacity of neural networks, by changing the architecture design from the hand-crafted manner to automation. Despite remarkable achievements being made on various benchmark tasks, the development of neural architectures still faces several challenges. On the one hand, current neural architecture designs are not fully automatic yet. For instance, even with NAS, we still require tremendous knowledge from human experts on designing the architecture search space, defining search strategies and selecting training hyperparameters. On the other hand, existing neural architectures are severely exposed to the problems of lacking interpretability, vulnerability to adversarial examples, incapability of abstract reasoning, etc.</p>

        <p>In this workshop, we will focus on recent research and future directions on advancing the deep learning system, particularly from the perspective of neural architectures. We aim to bring experts from artificial intelligence, machine learning, deep learning, statistics, computer vision, and cognitive science communities together not only on discussing the current challenges of neural architecture designs, but also on charting out the blueprint of neural architectures for further bridging the gap between the human brain and neural networks.</p>

    </div>
</div>

</br>


<div class="container">
    <h2>Schedule</h2>
<!--    <h3><a href=" ">[YouTube Recording]</a > </h3>-->
<!--    <h3><strong>Please <a href="https://app.sli.do/event/0iocoe6a">ask and vote panel questions</a > ahead</strong></h3>-->
    <div class="schedule">
        <p><strong>08:55 - 09:00 &nbsp &nbsp &nbsp &nbsp Opening Remark</strong></p>
        <p><strong>09:00 - 09:35 &nbsp  &nbsp &nbsp &nbsp Talk 1: Jingdong Wang -- Dense Prediction with Transformers: Semantic Segmentation and High-Resolution Backbone</p>
        <p><strong>09:35 - 10:10 &nbsp  &nbsp &nbsp &nbsp <a href="talks/NAS_Yuille.pdf">Talk 2: Alan Yuille -- Towards Bayesian Generative Architectures</a></p>
        <p><strong>10:10 - 10:45 &nbsp &nbsp  &nbsp &nbsp Talk 3: Frank Hutter -- Neural Architecture Search (NAS) Benchmarks: Successes & Challenges</p>

        <p><strong>10:45 - 12:00 &nbsp &nbsp  &nbsp &nbsp <a href="resources/neural-PPF.pdf">Poster Session</a></p><br>

        <p><strong>12:20 - 14:00 &nbsp &nbsp  &nbsp &nbsp Lunch Break</strong></p><br>

        <p><strong>14:00 - 14:35 &nbsp &nbsp  &nbsp &nbsp Talk 4: David Kristjanson Duvenaud -- Learning to Skip the Boring Details</p>
        <p><strong>14:35 - 15:10 &nbsp &nbsp  &nbsp &nbsp Talk 5: Anima Anandkumar -- Are Transformers the Future of Vision?</p>
        <p><strong>15:10 - 15:45 &nbsp &nbsp  &nbsp &nbsp Talk 6: Been Kim -- Interpretability for (somewhat) Philosophical and Skeptical Minds</a></p>
        <p><strong>15:45 - 16:20 &nbsp &nbsp  &nbsp &nbsp Talk 7: Hanxiao Liu -- Towards Automated Design of ML Building Blocks</p>
    </div>
</div>
</br>

<div class="container">
    <h2>Instruction</h2>
    <div class="overview">
        On the workshop day (Monday, October 11th), you could go to <a href="https://workshopsdayone.event.gatherly.io">https://workshopsdayone.event.gatherly.io</a> to present your poster. The authors are supposed to be at the poster for their paper (please find the ID at <a href="resources/neural-PPF.pdf">here</a>). When attendees come, the authors can share the screen to show the poster details and answer questions.
        </br></br>
        Optional but highly encouraged: If you would like to play with this system beforehand, please visit <a href="https://workshopsdayonetesting.event.gatherly.io">https://workshopsdayonetesting.event.gatherly.io</a>.
        </br></br>
        Note: for the poster and video preparation, please follow <a href="https://iccv2021.thecvf.com/paper-presentation-requirements-iccv-2021"> the instructions from the ICCV main website</a>.
    </div>
</div>
</br>

<!-- 
<div class="container">
    <h2>Call For Papers</h2>
    <div class="call4papers">
        <p><strong>Submission Deadline</strong>:&nbsp; <strong>August 5, 2021</strong> Anywhere on Earth (AoE)</p>
        <p><strong>Notification Sent to Authors</strong>:&nbsp; <strong>August 13, 2021</strong> Anywhere on Earth (AoE)</p>
        <p><strong>Camera Ready Deadline</strong>: &nbsp; <strong>August 16, 2021</strong> Anywhere on Earth (AoE)</p>
        <p><strong>Submission Server</strong>:&nbsp; <a href="https://cmt3.research.microsoft.com/NeurArch2021">https://cmt3.research.microsoft.com/NeurArch2021</a></p>
        <p><strong>Submission Format</strong>:&nbsp; Submissions need to be anonymized and follow the <a href="http://iccv2021.thecvf.com/node/4#submission-guidelines">ICCV 2021 Author Instructions</a>. Please use the <a href="http://iccv2021.thecvf.com/sites/default/files/2020-09/iccv2021AuthorKit.zip">ICCV 2021 template</a> for the paper preparation.
        <p><strong>Submission Type</strong>:
        <table border="2">
        <tbody>
            <tr>
                <td style="padding:0 15px 0 15px;"> </td>
                <td style="padding:0 15px 0 15px;">Long Paper</td>
                <td style="padding:0 15px 0 15px;">Extended Abstract</td>
            </tr>
            <tr>
                <td style="padding:0 15px 0 15px;">Main Part (limited to)</td>
                <td style="padding:0 15px 0 15px;">8 pages</td>
                <td style="padding:0 15px 0 15px;">4 pages</td>
            </tr>
            <tr>
                <td style="padding:0 15px 0 15px;">References</td>
                <td style="padding:0 15px 0 15px;">no limitation</td>
                <td style="padding:0 15px 0 15px;">no limitation</td>
            </tr>
            <tr>
                <td style="padding:0 15px 0 15px;">ICCV Proceeding</td>
                <td style="padding:0 15px 0 15px;">include</td>
                <td style="padding:0 15px 0 15px;">NOT include</td>
            </tr>
        </tbody>
        </table>
                <br>
                The workshop considers two types of submissions:
                <ul>
                <li> (1) Long Paper: Papers are limited to 8 pages excluding references and will be included in the official ICCV proceedings</li>
                <li> (2) Extended Abstract: Papers are limited to 4 pages excluding references and will NOT be included in the official ICCV proceedings.</li>
                </ul>
        Based on the PC recommendations, the accepted long papers/extended abstracts will be allocated either a contributed talk or a poster presentation.</p>
        
        <p><strong>Scope</strong>:&nbsp; We invite submissions on <strong>any aspect of neural architectures</strong>. 
        This includes, but is not limited to:</p>
        <ul>
            <li> Understanding of neural architecture designs for optimization </li>
            <li> Novel frameworks of neural architecture search </li>
            <li> Reproducibility in neural architecture search </li>
            <li> Efficient/Robust/Explainable neural architectures </li>
            <li> Biologically inspired learning frameworks </li>
            <li> Applications of novel neural architectures </li>
        </ul>

        <p><strong>Double Blind Review</strong>:&nbsp; ICCV reviewing is double blind, in that authors do not know the names of the area chair/reviewers of their papers, and the area chairs/reviewers cannot, beyond reasonable doubt, infer the names of the authors from the submission and the additional material. Avoid providing information that may identify the authors in the acknowledgments (e.g., co-workers and grant IDs) and in the supplemental material (e.g., titles in the movies, or attached papers). Avoid providing links to websites that identify the authors. Violation of any of these guidelines may lead to rejection without review. If you need to cite a different paper of yours that is being submitted concurrently to ICCV, the authors should (1) cite these papers; (2) argue in the body of your paper why your ICCV paper is non trivially different from these concurrent submissions; and (3) include anonymized versions of those papers in the supplemental material.
    </div>
</div>
</br> -->

<div class="container">
        <h2>Accepted Long Paper (<a href="https://openaccess.thecvf.com/ICCV2021_workshops/NeurArch">Proceeding</a>)</h2>
        <div class="overview">
            <ul>
            <li><b> SCARLET-NAS: Bridging the Gap between Stability and Scalability in Weight-sharing Neural Architecture Search </b>[<a href="https://openaccess.thecvf.com/content/ICCV2021W/NeurArch/papers/Chu_SCARLET-NAS_Bridging_the_Gap_Between_Stability_and_Scalability_in_Weight-Sharing_ICCVW_2021_paper.pdf">Paper</a>]
                <br> Xiangxiang Chu (Meituan), Bo Zhang (Meituan)*, QINGYUAN LI (), Ruijun Xu (), Xudong Li (Chinese Academy of Sciences) </li>
            <li><b> CONet: Channel Optimization for Convolutional Neural Networks </b> [<a href="https://openaccess.thecvf.com/content/ICCV2021W/NeurArch/papers/Hosseini_CONet_Channel_Optimization_for_Convolutional_Neural_Networks_ICCVW_2021_paper.pdf">Paper</a>]
                <br> Mahdi S. Hosseini (University of New Brunswick)*, Jia Shu Zhang (University of Toronto), Zhe M Liu (University of Toronto), Andre Fu (University of Toronto), Jingxuan Su (University of Toronto), Mathieu Tuli (University of Toronto), Konstantinos N Plataniotis (University of Toronto) </li>
            <li><b> Russian Doll Network: Learning Nested Networks for Sample-Adaptive Dynamic Inference </b> [<a href="https://openaccess.thecvf.com/content/ICCV2021W/NeurArch/papers/Jiang_Russian_Doll_Network_Learning_Nested_Networks_for_Sample-Adaptive_Dynamic_Inference_ICCVW_2021_paper.pdf">Paper</a>]
                <br> Borui Jiang (Peking University)*, Yadong Mu (Peking University) </li>
            <li><b> Tiled Squeeze-and-Excite: Channel Attention With Local Spatial Context</b> [<a href="https://openaccess.thecvf.com/content/ICCV2021W/NeurArch/papers/Vosco_Tiled_Squeeze-and-Excite_Channel_Attention_With_Local_Spatial_Context_ICCVW_2021_paper.pdf">Paper</a>] [<a href="https://www.youtube.com/watch?v=Ajqmk0jj55g&t=20s">Video</a>] 
                <br> Niv Vosco (Hailo)*, Alon Shenkler (Hailo), Mark Grobman (Hailo) </li>
            <li><b> DDUNet: Dense Dense U-Net with Applications in Image Denoising</b> [<a href="https://openaccess.thecvf.com/content/ICCV2021W/NeurArch/papers/Jia_DDUNet_Dense_Dense_U-Net_With_Applications_in_Image_Denoising_ICCVW_2021_paper.pdf">Paper</a>] [<a href="https://www.youtube.com/watch?v=0NF2HXgPQY0">Video</a>] [<a href="posters/05-DDUnet.pdf">Poster</a>]
                <br> Fan JIA (The Chinese University of Hong Kong), Wing Hong Wong (The Chinese University of Hong Kong), Tieyong Zeng (The Chinese University of Hong Kong)* </li>
            <li><b> PP-NAS: Searching for Plug-and-Play Blocks on Convolutional Neural Network</b> [<a href="https://openaccess.thecvf.com/content/ICCV2021W/NeurArch/papers/Shen_PP-NAS_Searching_for_Plug-and-Play_Blocks_on_Convolutional_Neural_Network_ICCVW_2021_paper.pdf">Paper</a>]
                <br> Biluo Shen (Chinese Academy of Sciences), Anqi Xiao (Chinese Academy of Sciences), Jie Tian (Chinese Academy of Sciences), Zhenhua Hu (Chinese Academy of Sciences)* </li>
            <li><b> Single-DARTS: Towards Stable Architecture Search </b> [<a href="https://openaccess.thecvf.com/content/ICCV2021W/NeurArch/papers/Hou_Single-DARTS_Towards_Stable_Architecture_Search_ICCVW_2021_paper.pdf">Paper</a>]
                <br> Pengfei hou (Alibaba)*, Ying Jin (Tsinghua University), Yukang Chen (The Chinese University of Hong Kong) </li>
            <li><b> Convolutional Filter Approximation Using Fractional Calculus</b> [<a href="https://openaccess.thecvf.com/content/ICCV2021W/NeurArch/papers/Zamora_Convolutional_Filter_Approximation_Using_Fractional_Calculus_ICCVW_2021_paper.pdf">Paper</a>] [<a href="https://www.youtube.com/watch?v=bxcRwjCF7ic">Video</a>]
                <br> Julio Zamora (Intel Labs)*, Jesus Adan Cruz Vargas (Intel Labs), Anthony D Rhodes (Intel Labs), Lama Nachman (Intel Labs), Narayan Sundararajan (Intel Labs) </li>
            <li><b> Graph-based Neural Architecture Search with Operation Embeddings</b> [<a href="https://openaccess.thecvf.com/content/ICCV2021W/NeurArch/papers/Chatzianastasis_Graph-Based_Neural_Architecture_Search_With_Operation_Embeddings_ICCVW_2021_paper.pdf">Paper</a>] [<a href="https://www.youtube.com/watch?v=-rZ4tpNvL6s">Video</a>]
                <br> "Michail Chatzianastasis (National Technical University of Athens)*, George Dasoulas (Ecole Polytechnique, France), Georgios Siolas (National Tecnhical University of Athens), Michalis Vazirgiannis (École Polytechnique)" </li>
            <li><b> Contextual Convolutional Neural Networks</b> [<a href="https://openaccess.thecvf.com/content/ICCV2021W/NeurArch/papers/Duta_Contextual_Convolutional_Neural_Networks_ICCVW_2021_paper.pdf">Paper</a>]
                <br> Ionut Cosmin Duta (University of Bucharest), Mariana-Iuliana Georgescu (University of Bucharest), Radu Tudor Ionescu (University of Bucharest)* </li>
            <li><b> Leveraging Batch Normalization for Vision Transformers</b> [<a href="https://openaccess.thecvf.com/content/ICCV2021W/NeurArch/papers/Yao_Leveraging_Batch_Normalization_for_Vision_Transformers_ICCVW_2021_paper.pdf">Paper</a>]
                <br> Zhuliang Yao (Tsinghua University), Yue Cao (Microsoft Research Asia)*, Yutong Lin (Xi'an Jiaotong University), Ze Liu (USTC), Zheng Zhang (MSRA, Huazhong University of Science and Technolog), Han Hu (Microsoft Research Asia) </li>
            </ul>
        </div>
</div>
<div class="container">
        <h2>Accepted Extended Abstract</h2>
        <div class="overview">
            <ul>
            <li><b> Searching for Efficient Multi-Stage Vision Transformers </b>[<a href="papers/00011.pdf">Paper</a>]<br>
            Yi-Lun Liao (Massachusetts Institute of Technology)*, Sertac Karaman (Massachusetts Institute of Technology), Vivienne Sze (Massachusetts Institute of Technology) </li> 
            <li><b> OSNASLib: One-Shot NAS Library </b>[<a href="papers/00010.pdf">Paper</a>]<br>
            Sian-Yao Huang (National Cheng Kung University)*,  Wei-Ta Chu (National Cheng Kung University) </li> 
            </ul>
        </div>
</div>
</br> 


<div class="container">
    <h2>Speakers</h2>
    <div>

        <div class="instructor">
            <a href="https://jingdongwang2017.github.io/">
                <div class="instructorphoto"><img src="speakers/jingdong-wang.jpeg"></div>
                <div>Jingdong Wang<br>Baidu Inc.</div>
            </a>
        </div>

        <div class="instructor">
            <a href="http://www.cs.jhu.edu/~ayuille/index.html">
                <div class="instructorphoto"><img src="speakers/alanyuille.png"></div>
                <div>Alan Yuille<br>Johns Hopkins University</div>
            </a>
        </div>

        <div class="instructor">
            <a href="http://ml.informatik.uni-freiburg.de/~hutter">
                <div class="instructorphoto"><img src="speakers/frank.jpeg"></div>
                <div>Frank Hutter<br>University of Freiburg</div>
            </a>
        </div>

        <p></p>


        <div class="instructor">
            <a href="http://www.cs.toronto.edu/~duvenaud/">
                <div class="instructorphoto"><img src="speakers/david-duvenaud-headshot.jpeg"></div>
                <div>David Duvenaud<br>University of Toronto</div>
            </a>
        </div>

        <div class="instructor">
            <a href="http://tensorlab.cms.caltech.edu/users/anima">
                <div class="instructorphoto"><img src="speakers/anima.jpg"></div>
                <div>Anima Anandkumar<br>Caltech & NVIDIA</div>
            </a>
        </div>

        <div class="instructor">
            <a href="https://beenkim.github.io/">
                <div class="instructorphoto"><img src="speakers/beenkim.jpeg"></div>
                <div>Been Kim<br>Google Brain</div>
            </a>
        </div>

        <div class="instructor">
            <a href="https://quark0.github.io">
                <div class="instructorphoto"><img src="speakers/hanxiao.jpeg"></div>
                <div>Hanxiao Liu<br>Google Brain</div>
            </a>
        </div>

    </div>
</div>

</br>

<div class="container">
    <h2>Organizing Committee</h2>
    <div>
        <div class="instructor">
            <a href="https://cihangxie.github.io/">
                <div class="instructorphoto"><img src="figures/cihangxie.jpg"></div>
                <div>Cihang Xie<br>UCSC</div> <!-- University of California, Santa Cruz -->
            </a>
        </div>

        <div class="instructor">
            <a href="https://xuanyidong.com/">
                <div class="instructorphoto"><img src="figures/xuanyidong.png"></div>
                <div>Xuanyi Dong<br>University of Technology Sydney</div>
            </a>
        </div>

        <div class="instructor">
            <a href="https://lingxixie.com/">
                <div class="instructorphoto"><img src="figures/lingxixie.jpg"></div>
                <div>Lingxi Xie<br>Huawei Inc.</div>
            </a>
        </div>

        <div class="instructor">
            <a href="https://xiaocw11.github.io">
                <div class="instructorphoto"><img src="figures/chaoweixiao.jpg"></div>
                <div>Chaowei Xiao<br>NVIDIA Research</div>
            </a>
        </div>

    </div>

    <p></p>
    
    <div>
        <div class="instructor">
            <a href="https://sites.google.com/site/azadenazi/">
                <div class="instructorphoto"><img src="figures/azadenazi.jpg"></div>
                <div>Azade Nazi<br>Google Brain</div>
            </a>
        </div>

        <div class="instructor">
            <a href="https://liulu112601.github.io/">
                <div class="instructorphoto"><img src="figures/luliu.jpg"></div>
                <div>Lu Liu<br>University of Technology Sydney</div>
            </a>
        </div>

        <div class="instructor">
            <a href="https://research.google/people/105445/">
                <div class="instructorphoto"><img src="figures/mingxingtan.png"></div>
                <div>Mingxing Tan<br>Google Brain</div>
            </a>
        </div>

        <div class="instructor">
            <a href="https://tfwu.github.io/">
                <div class="instructorphoto"><img src="figures/tianfuwu.jpg"></div>
                <div>Tianfu Wu<br>NC State University</div>
            </a>
        </div>

        <div class="instructor">
            <a href="https://cs.stanford.edu/~quocle/">
                <div class="instructorphoto"><img src="figures/quocle.png"></div>
                <div>Quoc V Le<br>Google Brain</div>
            </a>
        </div>
    </div>
</div>
</br>

<!-- 
<div class="container">
    <h2>Program Committee</h2>
    TBD
    <div class="pcs-row pcs">
        <div class="pcs-column">
            <ul>
                <li>Maksym Andriushchenko (EPFL)</li>
                <li>Anurag Arnab (Google)</li>
                <li>Arjun Nitin Bhagoji (Princeton University)</li>
                <li>Wieland Brendel (University of Tübingen)</li>
                <li>Yulong Cao (University of Michigan)</li>
                <li>Hongge Chen (MIT)</li>
                <li>Ambra Demontis (University of Cagliari)</li>
                <li>Yinpeng Dong (Tsinghua University)</li>
                <li>Sven Gowal (DeepMind)</li>
                <li>Chuan Guo (Cornell University)</li>
                <li>Saumya Jetley (INRIA)</li>
                <li>Adam Kortylewski (Johns Hopkins University)</li>
                <li>Alexey Kurakin (Google Brain)</li>
                <li>Yingwei Li (Johns Hopkins University)</li>
                <li>Jingyue Lu (University of Oxford)</li>
                <li>Jan Hendrik Metzen (Bosch Center for Artificial Intelligence)</li>
                <li>Mahyar Najibi (University of Maryland, College Park)</li>
                <li>Tianyu Pang (Tsinghua University)</li>
                <li>Maura Pintor (University of Cagliari)</li>
            </ul>
        </div>
        <div class="pcs-column">
            <ul>
                <li>Hamed Pirsiavash (UMBC)</li>
                <li>Omid Poursaeed (Cornell University)</li>
                <li>Aaditya Prakash (PathAI)</li>
                <li>Chongli Qin (DeepMind)</li>
                <li>Jonas Rauber (University of Tübingen)</li>
                <li>Aniruddha Saha (UMBC)</li>
                <li>Ali Shafahi (University of Maryland, College Park)</li>
                <li>Yash Sharma (University of Tübingen)</li>
                <li>Akshayvarun Subramanya (UMBC)</li>
                <li>Krishna Kumar Singh (UC Davis)</li>
                <li>David Stutz (Max Planck Institute for Informatics)</li>
                <li>Peng Tang (Salesforce Research)</li>
                <li>Jianyu Wang (Waymo)</li>
                <li>Yuxin Wu (Facebook AI Research)</li>
                <li>Chang Xiao (Columbia University)</li>
                <li>Chaowei Xiao (University of Michigan)</li>
                <li>Hongyang Zhang (Toyota Technological Institute at Chicago)</li>
                <li>Huan Zhang (UCLA)</li>
                <li>Dan Xu (University of Oxford)</li>    
            </ul>
        </div>
    </div>
</div>
</br> -->

<!-- <div class="container">
    <h2>Sponsor</h2>
    TBD
    
    <div><img width="350" src="figures/DeepMind_RGB_Lockup_LogoHiRes_Blue.png"></div>
    
</div> -->

</br>

<div class="containersmall">
    <p>Please <a href="mailto:neural.architecture.workshop@gmail.com">contact us</a> if you have questions.</p>
</div>

<!--<p align="center" class="acknowledgement">Last updated: Jan. 6, 2017</p>-->
</body>
</html>
